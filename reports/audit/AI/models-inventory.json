{
  "timestamp": "2025-10-10T19:08:13.650Z",
  "local_models": [
    {
      "name": "qwen2-1_5b-instruct-q4_k_m.gguf",
      "type": "local",
      "format": "gguf",
      "size_gb": 0,
      "device": "cpu",
      "dimensions": 768,
      "license": "unknown",
      "last_modified": "2025-10-09T10:18:02.486Z",
      "latency_p50_ms": 65.79641092665456,
      "latency_p95_ms": 0
    }
  ],
  "cached_models": [],
  "remote_models": [
    {
      "name": "Xenova/multilingual-e5-small",
      "type": "transformers",
      "provider": "huggingface",
      "format": "onnx",
      "size_gb": 0.1,
      "device": "cpu",
      "dimensions": 384,
      "license": "mit",
      "latency_p50_ms": 25,
      "latency_p95_ms": 45,
      "usage": "embeddings",
      "quantized": true
    },
    {
      "name": "qwen2-1_5b-instruct-q4_k_m",
      "type": "local",
      "provider": "alibaba",
      "format": "gguf",
      "size_gb": 0.9,
      "device": "cpu",
      "dimensions": 1536,
      "license": "apache-2.0",
      "latency_p50_ms": 800,
      "latency_p95_ms": 1500,
      "usage": "text_generation",
      "quantized": true,
      "parameters": "1.5B"
    }
  ],
  "summary": {
    "total_models": 3,
    "total_size_gb": 1,
    "avg_latency_ms": 297
  },
  "recommendations": [
    "High average latency - optimize model inference"
  ]
}