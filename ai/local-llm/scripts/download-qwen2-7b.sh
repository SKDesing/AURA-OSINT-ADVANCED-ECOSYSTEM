#!/usr/bin/env bash
set -euo pipefail

MODEL_DIR="ai/local-llm/models"
MODEL_FILE="qwen2-7b-instruct-q5_k_m.gguf"
URL="https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/qwen2-7b-instruct-q5_k_m.gguf"

mkdir -p "$MODEL_DIR"

echo "üîΩ T√©l√©chargement Qwen2 7B Instruct (Q5_K_M) - ~5GB..."
echo "‚ö†Ô∏è  Ce t√©l√©chargement peut prendre 10-30 minutes selon votre connexion"

if [[ ! -f "$MODEL_DIR/$MODEL_FILE" ]]; then
    # V√©rifier l'espace disque (besoin de ~6GB)
    AVAILABLE_GB=$(df "$MODEL_DIR" | tail -1 | awk '{print int($4/1024/1024)}')
    if [[ $AVAILABLE_GB -lt 6 ]]; then
        echo "‚ùå Espace disque insuffisant: ${AVAILABLE_GB}GB disponible, 6GB requis"
        exit 1
    fi
    
    echo "üíæ Espace disponible: ${AVAILABLE_GB}GB"
    echo "üì• T√©l√©chargement en cours..."
    
    # T√©l√©chargement avec barre de progression
    if curl --help | grep -q progress-bar; then
        CURL_OPTS="--progress-bar"
    else
        CURL_OPTS="-#"
    fi
    
    # T√©l√©chargement avec reprise automatique
    curl -L $CURL_OPTS -C - -o "$MODEL_DIR/$MODEL_FILE" "$URL"
    
    echo "‚úÖ Mod√®le t√©l√©charg√©: $MODEL_DIR/$MODEL_FILE"
else
    echo "‚ÑπÔ∏è  Mod√®le d√©j√† pr√©sent: $MODEL_DIR/$MODEL_FILE"
fi

echo "üîê G√©n√©ration hash SHA256..."
ACTUAL_SHA256=$(sha256sum "$MODEL_DIR/$MODEL_FILE" | cut -d' ' -f1)
echo "$ACTUAL_SHA256" > "$MODEL_DIR/$MODEL_FILE.sha256"

ACTUAL_SIZE=$(du -h "$MODEL_DIR/$MODEL_FILE" | cut -f1)
echo "üìä Taille: $ACTUAL_SIZE"
echo "üîë Hash: $ACTUAL_SHA256"
echo "üìù Hash sauvegard√©: $MODEL_DIR/$MODEL_FILE.sha256"

# V√©rifier que le mod√®le n'est pas dans Git
echo "üõ°Ô∏è  V√©rification exclusion Git..."
if git check-ignore "$MODEL_DIR/$MODEL_FILE" >/dev/null 2>&1; then
    echo "‚úÖ Mod√®le exclu de Git (s√©curis√©)"
else
    echo "‚ö†Ô∏è  ATTENTION: Mod√®le pourrait √™tre commit√© dans Git!"
    echo "   V√©rifiez votre .gitignore"
fi

echo "‚úÖ Qwen2 7B pr√™t pour utilisation"
echo ""
echo "üöÄ Prochaines √©tapes:"
echo "   1. Installer llama.cpp: bash scripts/ai/install-llama-cpp.sh"
echo "   2. D√©marrer serveur: AI_QWEN_MODEL_FILE=$MODEL_DIR/$MODEL_FILE bash ai/local-llm/scripts/run-llm-qwen.sh"
echo "   3. Tester: curl -X POST http://127.0.0.1:8090/completion -H 'Content-Type: application/json' -d '{\"prompt\":\"Hello\"}'"